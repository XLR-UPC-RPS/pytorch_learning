import torch
import torch.nn as nn
import torchvision.models as models
from torchvision import transforms
from PIL import Image
import os
import clip
# ==================== 1. å®šä¹‰VisionTokenizeræ¨¡å‹ï¼ˆä¿®æ­£ç‰ˆï¼‰ ====================
max_T = 32
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class VisionTokenizer(nn.Module):
    def __init__(self, d_model=512):
        super().__init__()
        # åˆ›å»ºResNet-18å¹¶åŠ è½½é¢„è®­ç»ƒæƒé‡
        resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        # å»æ‰avgpoolå’Œfcå±‚ï¼Œä¿ç•™ç‰¹å¾æå–éƒ¨åˆ†
        self.backbone = nn.Sequential(*list(resnet18.children())[:-2])
        # 1x1å·ç§¯æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
        self.conv_proj = nn.Conv2d(512, d_model, kernel_size=1)
        # æ—¶é—´ä½ç½®ç¼–ç 
        self.time_embed = nn.Embedding(max_T, d_model)

    def forward(self, images):
        """
        images: (B, T, 3, H, W)  B=æ‰¹æ¬¡, T=æ—¶åºé•¿åº¦, 3=é€šé“, H/W=å›¾åƒå°ºå¯¸
        return: (B, T, N, D)     N=å›¾åƒTokenæ•°, D=Tokenç»´åº¦
        """
        B, T, C, H, W = images.shape
        # æŠ˜å æ—¶åºç»´åº¦åˆ°æ‰¹æ¬¡ç»´åº¦ï¼Œé€‚é…ResNetè¾“å…¥
        x = images.view(B * T, C, H, W)

        # æå–è§†è§‰ç‰¹å¾
        feat = self.backbone(x)  # (B*T, 512, H', W')
        feat = self.conv_proj(feat)  # (B*T, D, H', W')

        # å±•å¹³ä¸ºTokenåºåˆ—
        D, Hp, Wp = feat.shape[1:]
        tokens = feat.flatten(2).transpose(1, 2)  # (B*T, N, D) N=Hp*Wp

        # æ¢å¤æ—¶åºç»´åº¦
        tokens = tokens.view(B, T, -1, D)

        # æ·»åŠ æ—¶é—´ä½ç½®ç¼–ç 
        time_ids = torch.arange(T, device=images.device)
        time_emb = self.time_embed(time_ids)  # (T, D)
        time_emb = time_emb.view(1, T, 1, D)  # å¹¿æ’­é€‚é…
        tokens = tokens + time_emb

        return tokens


#====================== LanguageTokenize =====================

class LanguageTokenizer(nn.Module):
    def __init__(self,d_model=512):
        super().__init__()

        clip_model,_ = clip.load("ViT-B/32",device=device)
        self.text_encoder = clip_model.encode_text

        self.proj = nn.Linear(512, d_model).to(device)

        #å†»ç»“CLIP
        for p in clip_model.parameters():
            p.requires_grad = False

    def forward(self, texts):
        """
        texts: list[str] of length B
        return: (B, D)
        """

        #å°†textçš„tokenè¿ç§»åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡ä¸Šï¼Œparameters()è¿”å›æ¨¡å‹çš„æ‰€æœ‰å¯è®­ç»ƒå‚æ•°ï¼Œnext()å–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆæ‰€æœ‰å…ƒç´ éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Šï¼Œå–ç¬¬ä¸€ä¸ªå°±å¥½ï¼‰
        tokens = clip.tokenize(texts).to(next(self.proj.parameters()).device)
        text_feat = self.text_encoder(tokens)   #(B,512)

        # ç¡®ä¿ text_feat ä¸ proj æƒé‡ç±»å‹ä¸€è‡´
        text_feat = text_feat.to(self.proj.weight.dtype)

        text_feat = self.proj(text_feat)        #(B,D)

        return text_feat

class LanguagePrefix(nn.Module):
    """
        Expand language embedding into K prefix tokens
        (B,D)â†’(B,K,D)
    """

    def __init__(self,num_tokens=4):
        super().__init__()
        self.num_tokens = num_tokens

    def forward(self, lang_feat):
        """
            lang_feat: (B, D)
            return: (B, K, D)
        """

        lang_feat = lang_feat.to(device)

        return lang_feat.unsqueeze(1).repeat(1, self.num_tokens, 1)     #é€šè¿‡unsqueeze(1)æ–°å¢ä¸€ä¸ªåºåˆ—ç»´åº¦ï¼Œå†é€šè¿‡repeatåœ¨è¿™ä¸ªæ–°å¢ç»´åº¦ä¸Šå¤åˆ¶self.num_tokensæ¬¡
        #æ‰€ä»¥ï¼Œè¿™ä¸ªnum_tokenæ˜¯å¤šå°‘æ¬¡å‘¢?
        #å®éªŒç»“æœï¼Œç»éªŒå‚æ•°

def build_rt1_input_sequence(lang_tokens,vision_tokens):
    """
        lang_tokens:   (B, K, D)
        vision_tokens: (B, T, N, D)

        return:
            full_seq: (B, K + T*N, D)
    """

    B,T,N,D = vision_tokens.shape

    vision_seq = vision_tokens.view(B,T*N,D).to(device)     #æ”¹å˜å¼ é‡shapeå³æ•°æ®æ’åˆ—æ–¹å¼ï¼Œä¸æ”¹å˜æ•°æ®
    full_seq = torch.cat([lang_tokens, vision_seq], dim=1)

    return full_seq


# ==================== 2. å›¾ç‰‡åŠ è½½ä¸é¢„å¤„ç†å‡½æ•° ====================
def load_and_preprocess_image(img_path):
    """
    åŠ è½½å•å¼ å›¾ç‰‡å¹¶è½¬æ¢æˆæ¨¡å‹è¦æ±‚çš„æ ¼å¼ï¼š
    1. æ‰“å¼€å›¾ç‰‡å¹¶è½¬ä¸ºRGBï¼ˆé¿å…ç°åº¦å›¾/é€æ˜é€šé“é—®é¢˜ï¼‰
    2. ç¼©æ”¾ã€è£å‰ªåˆ°224x224ï¼ˆResNetæ ‡å‡†è¾“å…¥ï¼‰
    3. å½’ä¸€åŒ–ï¼ˆç¬¦åˆImageNeté¢„è®­ç»ƒçš„å‡å€¼/æ–¹å·®ï¼‰
    4. è½¬æ¢æˆå¼ é‡å¹¶è°ƒæ•´ç»´åº¦ä¸º(1, 1, 3, 224, 224) â†’ (B=1, T=1, 3, H, W)
    """
    # å®šä¹‰ResNeté¢„è®­ç»ƒè¦æ±‚çš„é¢„å¤„ç†æµç¨‹
    preprocess = transforms.Compose([
        transforms.Resize(256),  # å…ˆç¼©æ”¾åˆ°256x256
        transforms.CenterCrop(224),  # ä¸­å¿ƒè£å‰ªåˆ°224x224
        transforms.ToTensor(),  # è½¬ä¸ºå¼ é‡ï¼ˆ0-1ï¼‰
        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNetå‡å€¼
                             std=[0.229, 0.224, 0.225])  # ImageNetæ–¹å·®
    ])

    # åŠ è½½å›¾ç‰‡ï¼ˆç¡®ä¿è·¯å¾„æ­£ç¡®ï¼‰
    if not os.path.exists(img_path):
        raise FileNotFoundError(f"å›¾ç‰‡è·¯å¾„ä¸å­˜åœ¨ï¼š{img_path}")
    img = Image.open(img_path).convert('RGB')  # è½¬ä¸ºRGBï¼Œé¿å…é€æ˜é€šé“

    # é¢„å¤„ç†å¹¶è°ƒæ•´ç»´åº¦
    img_tensor = preprocess(img)  # å½¢çŠ¶ï¼š(3, 224, 224)
    # æ‰©å±•ä¸º(B=1, T=1, 3, 224, 224) â†’ é€‚é…æ¨¡å‹è¾“å…¥æ ¼å¼
    img_tensor = img_tensor.unsqueeze(0).unsqueeze(0)

    return img_tensor


# ==================== 3. å•å¼ å›¾ç‰‡éªŒè¯ä¸»æµç¨‹ ====================
if __name__ == "__main__":
    # ===== é…ç½®å‚æ•° =====
    img_path = "/home/xlr/outputs/captured_images/opencv__dev_video2.png"  # æ›¿æ¢æˆä½ çš„å›¾ç‰‡è·¯å¾„ï¼ˆå¦‚ï¼š./cat.jpgã€/Users/xxx/photo.pngï¼‰
    d_model = 512  # Tokenç»´åº¦ï¼Œå’Œæ¨¡å‹å®šä¹‰ä¸€è‡´

    # ===== åˆå§‹åŒ–æ¨¡å‹ =====
    tokenizer = VisionTokenizer(d_model=d_model)
    tokenizer.eval()  # è¯„ä¼°æ¨¡å¼ï¼Œå…³é—­BatchNorm/Dropout

    # ===== åŠ è½½å¹¶é¢„å¤„ç†å›¾ç‰‡ =====
    try:
        input_tensor = load_and_preprocess_image(img_path)
        print(f"âœ… å›¾ç‰‡åŠ è½½æˆåŠŸï¼Œè¾“å…¥å¼ é‡å½¢çŠ¶ï¼š{input_tensor.shape}")
    except Exception as e:
        print(f"âŒ å›¾ç‰‡åŠ è½½å¤±è´¥ï¼š{e}")
        exit()

    # ===== æ¨¡å‹æ¨ç†ï¼ˆç¦ç”¨æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜ï¼‰ =====
    with torch.no_grad():
        output_tokens = tokenizer(input_tensor)

    # ===== éªŒè¯è¾“å‡ºæ˜¯å¦åˆç† =====
    print("=" * 60)
    # 1. éªŒè¯è¾“å‡ºå½¢çŠ¶
    B, T, N, D = output_tokens.shape

    print(f"ğŸ“Š è¾“å‡ºTokenå½¢çŠ¶ï¼š(B={B}, T={T}, N={N}, D={D})")

    texts = ["pick up the cube"]

    lang_encoder = LanguageTokenizer(d_model=D)

    prefixer = LanguagePrefix(num_tokens=4)

    lang_feat = lang_encoder(texts)  # (B, D)
    lang_tokens = prefixer(lang_feat)  # (B, 4, D)

    vision_tokens = torch.randn(B, T, N, D)

    full_seq = build_rt1_input_sequence(lang_tokens, vision_tokens)

    print("Language tokens:", lang_tokens.shape)
    print("Vision tokens:", vision_tokens.shape)
    print("Full sequence:", full_seq.shape)

    # # ResNet18å¯¹224x224å›¾ç‰‡ä¸‹é‡‡æ ·32å€ â†’ 7x7=49ä¸ªTokenï¼Œé¢„æœŸN=49
    # expected_N = (224 // 32) * (224 // 32)
    # print(f"âœ… é¢„æœŸTokenæ•°é‡Nï¼š{expected_N}ï¼Œå®é™…ï¼š{N} â†’ {'ç¬¦åˆ' if N == expected_N else 'ä¸ç¬¦åˆ'}")
    # print(f"âœ… é¢„æœŸTokenç»´åº¦Dï¼š{d_model}ï¼Œå®é™…ï¼š{D} â†’ {'ç¬¦åˆ' if D == d_model else 'ä¸ç¬¦åˆ'}")
    #
    # # 2. éªŒè¯æ•°å€¼æ˜¯å¦æ­£å¸¸ï¼ˆæ— NaN/Infï¼‰
    # has_nan = torch.isnan(output_tokens).any().item()
    # has_inf = torch.isinf(output_tokens).any().item()
    # print(f"âŒ è¾“å‡ºåŒ…å«NaNï¼š{has_nan} | âŒ è¾“å‡ºåŒ…å«Infï¼š{has_inf}")
    #
    # # 3. è¾“å‡ºTokençš„åŸºç¡€ç»Ÿè®¡ä¿¡æ¯ï¼ˆå‚è€ƒï¼‰
    # print(f"ğŸ“ˆ Tokenæ•°å€¼èŒƒå›´ï¼š[{output_tokens.min().item():.4f}, {output_tokens.max().item():.4f}]")
    # print(f"ğŸ“ˆ Tokenå‡å€¼ï¼š{output_tokens.mean().item():.4f} | æ ‡å‡†å·®ï¼š{output_tokens.std().item():.4f}")
    # print("=" * 60)
    #
    # # å¯é€‰ï¼šæ‰“å°å‰2ä¸ªTokençš„å‰5ä¸ªç»´åº¦å€¼ï¼ˆç›´è§‚æŸ¥çœ‹ï¼‰
    # print(f"ğŸ” å‰2ä¸ªTokençš„å‰5ä¸ªç»´åº¦å€¼ï¼š")
    # print(output_tokens[0, 0, :2, :5])
