import torch
import torch.nn as nn
import torchvision.models as models

import clip

max_T = 32

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class VisionTokenizer(nn.Module):
    def __init__(self,d_model=512):
        super().__init__()

        #åˆ›å»ºä¸€ä¸ªResNet-ç½‘ç»œå¹¶ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)

        # å»æ‰ avgpool å’Œ fc
        #resnet.children()è¿”å›ResNetçš„æ‰€æœ‰å±‚ï¼Œ[ï¼š-2]å»æ‰æœ€åä¸¤ä¸ªï¼ˆavgpool + fcï¼‰ï¼Œnn.Sequentioal()æŠŠå‰©ä¸‹æ‹¼æ¥æˆä¸€ä¸ªæ–°çš„ç½‘ç»œ
        #ResNetçš„æ‰€æœ‰å±‚ï¼šconv1 â†’ bn â†’ relu â†’ maxpool â†’ layer1 â†’ layer2 â†’ layer3 â†’ layer4 â†’ avgpool â†’ fc
        #å»æ‰æœ€åä¸¤ä¸ªçš„åŸå› ï¼šavgpoolä¼šæŠŠç©ºé—´å‹æ‰ï¼Œä¸¢å¤±ç©ºé—´ä¿¡æ¯ï¼Œfcä¼šå¼ºåˆ¶å˜æˆåˆ†ç±»
        self.backbone = nn.Sequential(*list(resnet.children())[:-2])
        #åšå·ç§¯æŠ•å½±,ä¿è¯æ‰€æœ‰çš„Tokenç»´åº¦ä¸€æ ·ï¼ˆCNNçš„è¾“å‡ºç»´åº¦å’ŒTransformerçš„ç»´åº¦å¯èƒ½ä¸ä¸€æ ·ï¼Œä¸è¿‡è¿™é‡Œæ˜¯ä¸€æ ·çš„ï¼‰
        self.conv_proj = nn.Conv2d(512, d_model, kernel_size=1)

        # ğŸ”‘ æ—¶é—´ positional embedding
        self.time_embed = nn.Embedding(max_T, d_model)

    def forward(self,images):
        """
               images: (B, T, 3, H, W)
               return: (B, T, N, D)

       | ç¬¦å·         | å«ä¹‰                   | ç›´è§‰è§£é‡Š                     |
        | ---------- | -------------------- | ------------------------ |
        | **B**      | Batch size           | ä¸€æ¬¡è®­ç»ƒç”¨å¤šå°‘æ¡è½¨è¿¹               |
        | **T**      | Time steps           | æ¯æ¡è½¨è¿¹æœ‰å¤šå°‘ä¸ªæ—¶é—´æ­¥              |
        | **C**      | Channels             | å›¾åƒé€šé“æ•°ï¼ˆRGB=3ï¼‰             |
        | **H, W**   | Height, Width        | åŸå§‹å›¾åƒåˆ†è¾¨ç‡                  |
        | **Hp, Wp** | Patch Height / Width | CNN è¾“å‡ºç‰¹å¾å›¾çš„ç©ºé—´å°ºå¯¸           |
        | **D**      | Embedding dim        | Transformer token çš„ç»´åº¦    |
        | **N**      | Num tokens           | æ¯ä¸€å¸§å›¾åƒçš„ token æ•° = Hp Ã— Wp |


        """
        B, T, C, H, W = images.shape
        x = images.view(B * T, C, H, W)     #ResNetä¸åŒºåˆ†æ—¶åºï¼Œæˆ‘ä»¬æŠŠæ—¶åºâ€œä¹˜â€ï¼ˆå¹¶éä¼ ç»Ÿæ„ä¹‰çš„ç›¸ä¹˜ï¼‰è¿›å»vï¼Œæ”¹å˜ç¼–ç æ–¹å¼ï¼Œçœ‹èµ·æ¥å°±åƒæ²¡æœ‰æ—¶åºä¸€æ ·
        #â€œåœ¨ RT-1 çš„ VisionTokenizer é‡Œï¼ŒB*T åªæ˜¯ä¸ºäº†å·¥ç¨‹ä¾¿åˆ©ï¼ŒæŠŠæ—¶é—´ç»´åº¦ä¸´æ—¶æŠ˜å æˆ batchï¼Œè§†è§‰æ¨¡å‹æœ¬èº«å®Œå…¨ä¸çŸ¥é“æ—¶é—´çš„å­˜åœ¨ã€‚â€
        feat = self.backbone(x)  # (B*T, 512, H', W')   3ç»è¿‡ResNet Backbone
        feat = self.conv_proj(feat) # (B*T, D, H', W')  æŠ•å½±åˆ°Transformerç»´åº¦


        D, Hp, Wp = feat.shape[1:]      #Hp, Wpæ˜¯ä¸‹é‡‡æ ·åçš„è¾“å‡ºâ€ç»´åº¦â€œ
        tokens = feat.flatten(2).transpose(1, 2)  # (B*T, N, D)     æŠŠ2Då›¾åƒå˜æˆtokenåºåˆ—

        tokens = tokens.view(B, T, -1, D)

        # ===== æ—¶é—´ positional embedding =====
        time_ids = torch.arange(T, device=images.device)  # (T,)
        time_emb = self.time_embed(time_ids)  # (T, D)
        time_emb = time_emb.view(1, T, 1, D)  # (1, T, 1, D)

        tokens = tokens + time_emb

        return tokens

class LanguageTokenizer(nn.Module):
    def __init__(self,d_model=512):
        super().__init__()

        clip_model,_ = clip.load("ViT-B/32",device=device)
        self.text_encoder = clip_model.encode_text

        self.proj = nn.Linear(512, d_model).to(device)

        #å†»ç»“CLIP
        for p in clip_model.parameters():
            p.requires_grad = False

    def forward(self, texts):
        """
        texts: list[str] of length B
        return: (B, D)
        """

        #å°†textçš„tokenè¿ç§»åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡ä¸Šï¼Œparameters()è¿”å›æ¨¡å‹çš„æ‰€æœ‰å¯è®­ç»ƒå‚æ•°ï¼Œnext()å–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆæ‰€æœ‰å…ƒç´ éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Šï¼Œå–ç¬¬ä¸€ä¸ªå°±å¥½ï¼‰
        tokens = clip.tokenize(texts).to(next(self.proj.parameters()).device)
        text_feat = self.text_encoder(tokens)   #(B,512)

        # ç¡®ä¿ text_feat ä¸ proj æƒé‡ç±»å‹ä¸€è‡´
        text_feat = text_feat.to(self.proj.weight.dtype)

        text_feat = self.proj(text_feat)        #(B,D)

        return text_feat

class LanguagePrefix(nn.Module):
    """
        Expand language embedding into K prefix tokens
        (B,D)â†’(B,K,D)
    """

    def __init__(self,num_tokens=4):
        super().__init__()
        self.num_tokens = num_tokens

    def forward(self, lang_feat):
        """
            lang_feat: (B, D)
            return: (B, K, D)
        """

        lang_feat = lang_feat.to(device)

        return lang_feat.unsqueeze(1).repeat(1, self.num_tokens, 1)     #é€šè¿‡unsqueeze(1)æ–°å¢ä¸€ä¸ªåºåˆ—ç»´åº¦ï¼Œå†é€šè¿‡repeatåœ¨è¿™ä¸ªæ–°å¢ç»´åº¦ä¸Šå¤åˆ¶self.num_tokensæ¬¡
        #æ‰€ä»¥ï¼Œè¿™ä¸ªnum_tokenæ˜¯å¤šå°‘æ¬¡å‘¢?
        #å®éªŒç»“æœï¼Œç»éªŒå‚æ•°

def build_rt1_input_sequence(lang_tokens,vision_tokens):
    """
        lang_tokens:   (B, K, D)
        vision_tokens: (B, T, N, D)

        return:
            full_seq: (B, K + T*N, D)
    """

    B,T,N,D = vision_tokens.shape

    vision_seq = vision_tokens.view(B,T*N,D).to(device)     #æ”¹å˜å¼ é‡shapeå³æ•°æ®æ’åˆ—æ–¹å¼ï¼Œä¸æ”¹å˜æ•°æ®
    full_seq = torch.cat([lang_tokens, vision_seq], dim=1)

    return full_seq

#å°†åŠ¨ä½œä»¤ç‰Œçš„æ•´æ•°ç¼–ç è½¬åŒ–ä¸º512ç»´çš„å‘é‡ï¼ˆç»´åº¦å’Œè§†è§‰è¯­è¨€ç¼–ç ä¸€è‡´ï¼‰
class ActionTokenizer(nn.Module):
    def __init__(self,action_vocab_size,d_model=512):
        super().__init__()
        self.action_embedding = nn.Embedding(action_vocab_size, d_model)

    def forward(self, actions_tokens):
        return self.action_embedding(actions_tokens)

def build_causal_mask(lang_len, vision_len, action_len, device):
    """

    è¯­è¨€ prefix tokens å’Œè§†è§‰ tokens éƒ½èƒ½è¢«ä»»ä½•ä½ç½®çœ‹åˆ°ï¼ˆæ²¡æ—¶é—´é¡ºåºé™åˆ¶ï¼‰

    åŠ¨ä½œ tokens ä¹‹é—´æŒ‰æ—¶é—´è‡ªå›å½’ï¼Œåªèƒ½çœ‹åˆ°å‰é¢çš„åŠ¨ä½œ token

    åŠ¨ä½œ tokens å¯ä»¥çœ‹åˆ°è¯­è¨€ + è§†è§‰ tokensï¼Œä½†è¯­è¨€ + è§†è§‰ tokens ä¸å—é™åˆ¶

    """
    total_len = lang_len + vision_len + action_len
    mask = torch.ones(total_len, total_len,device=device).tril()    #æ‰€æœ‰çš„tokenéƒ½èƒ½å¤Ÿçœ‹åˆ°è‡ªå·±

    mask[:lang_len + vision_len, :lang_len + vision_len] = 1       #æ‰€æœ‰çš„è¯­è¨€tokenå’Œvisiontkoenä¹‹é—´éƒ½èƒ½å¤Ÿç›¸äº’çœ‹åˆ°

    mask[lang_len + vision_len:, :lang_len + vision_len] = 1        #aciontokenèƒ½å¤Ÿçœ‹åˆ°æ‰€æœ‰çš„langtokenå’Œvisontoken

    return mask

class RT1Transformer(nn.Module):
    def __init__(self, d_model, nhead, num_layers, action_vocab_size):
        super().__init__()
        #from torch.nn import TransformerDecoder, TransformerDecoderLayer
        from torch.nn import TransformerEncoder, TransformerEncoderLayer

        # self.transformer_layer = TransformerDecoderLayer(d_model=d_model, nhead=nhead)
        # self.transformer = TransformerDecoder(self.transformer_layer, num_layers=num_layers)
        # #num_layerçš„å¤§å°æ˜¯å¦‚ä½•ç¡®å®šçš„ï¼Œè¶…å‚æ•°
        #
        # self.action_tokenizer = ActionTokenizer(action_vocab_size,d_model)
        # self.output_linear = nn.Linear(d_model, action_vocab_size)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=4 * d_model,
            batch_first=True
        )

        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )

        self.action_embed = nn.Embedding(action_vocab_size, d_model)
        self.head = nn.Linear(d_model, action_vocab_size)

    def forward(self, lang_tokens, vision_tokens, action_input_tokens):
        """
                lang_tokens: (B, K, D)
                vision_tokens: (B, T*N, D)
                action_input_tokens: (B, A, )  # int tokens

                returns:
                    logits over action vocab (B, A, action_vocab_size)      Aæ˜¯å•ä¸ªè½¨è¿¹åŠ¨ä½œtokençš„ä¸ªæ•°
        """
        # B = lang_tokens.size(0)
        # device = lang_tokens.device
        #
        # action_emb = self.action_tokenizer(action_input_tokens)     #(B,A,D)

        # #æ‹¼æ¥æ‰€æœ‰token
        # src = torch.cat([lang_tokens, vision_tokens, action_emb], dim=1)
        #
        # #æ„é€ mask
        # lang_len = lang_tokens.size(1)
        # vision_len = vision_tokens.size(1)
        # action_len = action_input_tokens.size(1)
        # mask = build_causal_mask(lang_len, vision_len, action_len, device)
        #
        # # transformer expects mask with False where attend allowed, True where blocked
        # attn_mask = ~mask.bool()    #è½¬åŒ–ä¸ºtransformerå…è®¸çš„å½¢å¼ï¼ŒæŠŠå…ƒçŸ©é˜µä¸­çš„1å˜æ¢ä¸ºTrue,0toFalse
        #
        # src = src.transpose(0, 1)       #äº¤æ¢Batchå’Œæ‹¼æ¥åçš„Tokené•¿åº¦çš„ä½ç½®ï¼ˆäº¤æ¢åæ‰ç¬¦åˆTransformerçš„è¦æ±‚ï¼‰
        # output = self.transformer(tgt=src, memory=None, tgt_mask=attn_mask)
        #
        # output = output.transpose(0, 1)
        # logits = self.output_linear(output[:, lang_len+vision_len:, :])
        #
        # return logits

        B = lang_tokens.size(0)
        device = lang_tokens.device

        action_emb = self.action_embed(action_input_tokens)

        src = torch.cat([lang_tokens, vision_tokens, action_emb], dim=1)

        lang_len = lang_tokens.size(1)
        vision_len = vision_tokens.size(1)
        action_len = action_input_tokens.size(1)

        attn_mask = build_causal_mask(lang_len, vision_len, action_len, device)

        out = self.transformer(src, mask=attn_mask)

        logits = self.head(out[:, lang_len + vision_len:])

        return logits


def sanity_forward_test():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    B = 2
    T = 4
    H = W = 224
    D = 512
    N = 49
    K = 4
    A = 6
    action_vocab = 20

    images = torch.randn(B, T, 3, H, W).to(device)
    texts = ["pick up cube", "grasp object"]
    actions = torch.randint(0, action_vocab, (B, A)).to(device)

    vision = VisionTokenizer(d_model=D).to(device)
    lang = LanguageTokenizer(d_model=D).to(device)
    prefix = LanguagePrefix(K).to(device)
    policy = RT1Transformer(D, 8, 4, action_vocab).to(device)

    with torch.no_grad():
        v_tokens = vision(images)            # (B,T,N,D)
        l_feat = lang(texts)                 # (B,D)
        l_tokens = prefix(l_feat)            # (B,K,D)
        v_seq = v_tokens.reshape(B, T*N, D)
        logits = policy(l_tokens, v_seq, actions)

    print("logits:", logits.shape)

if __name__ == "__main__":
    sanity_forward_test()